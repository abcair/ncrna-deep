{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation of tables and figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "v_acc_d = {}\n",
    "\n",
    "f = open('results/Rfam2Novel_constant.pckl', 'rb')\n",
    "v_acc_d.update({'Constant' : pickle.load(f)})\n",
    "f.close()\n",
    "\n",
    "f = open('results/Rfam2Novel_random.pckl', 'rb')\n",
    "v_acc_d.update({'Random' : pickle.load(f)})\n",
    "f.close()\n",
    "\n",
    "f = open('results/Rfam2Novel_new.pckl', 'rb')\n",
    "v_acc_d.update({'New' : pickle.load(f)})\n",
    "f.close()\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "train_labels=np.load(\"train_labels.npy\")\n",
    "val_labels=np.load(\"val_labels.npy\")\n",
    "test_labels=np.load(\"test_labels.npy\")\n",
    "\n",
    "num_classes = len(np.unique(train_labels))\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(train_labels)\n",
    "train_labels_num = le.transform(train_labels)\n",
    "val_labels_num = le.transform(val_labels)\n",
    "test_labels_num = le.transform(test_labels)\n",
    "\n",
    "train_labels_bin = keras.utils.to_categorical(train_labels_num, num_classes)\n",
    "val_labels_bin = keras.utils.to_categorical(val_labels_num, num_classes)\n",
    "test_labels_bin = keras.utils.to_categorical(test_labels_num, num_classes)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots the accuracy/MCC vs boundary noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plots accuracy or MCC VS bnoise\n",
    "from sklearn.metrics import *\n",
    "from utils.ExpConfiguration import *\n",
    "from math import sqrt\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "y_true = np.argmax(test_labels_bin, axis=1)\n",
    "\n",
    "# CHANGE HERE plot parameters\n",
    "nl=3  # cnn layer to plot\n",
    "#padd = 'Constant'\n",
    "#padd = 'Random'\n",
    "padd = 'New' \n",
    "#pmetric = 'MCC'\n",
    "#pmetricf = matthews_corrcoef\n",
    "pmetric = 'ACC'\n",
    "pmetricf = accuracy_score\n",
    "#pmetric = 'F1'\n",
    "#def f11_score(x,y): \n",
    "#    return f1_score(x,y,average='macro')\n",
    "\n",
    "#pmetricf = f11_score\n",
    "#pmetric = 'KAPPA'\n",
    "#pmetricf = cohen_kappa_score\n",
    "\n",
    "v_acc = v_acc_d[padd]\n",
    "fig, ax = plt.subplots()\n",
    "plt.xlabel('Boundary noise')\n",
    "plt.ylabel(pmetric)\n",
    "#plt.title('CNN n. layers = '+str(nl))\n",
    "plt.ylim(0.5, 1) \n",
    "for en in seqEncoders:\n",
    "    mtr=[]\n",
    "    mtrErr=[]\n",
    "    for bn in bnoise:\n",
    "        y_pred = v_acc[str(nl)][en['filename']][str(bn)]\n",
    "        #print('%.3f' % interval)\n",
    "        mtr.append(pmetricf(y_true,y_pred))\n",
    "        #mtrErr.append(1.96 * sqrt( (pmetricf(y_true,y_pred) * (1 - pmetricf(y_true,y_pred))) / len(y_pred)))\n",
    "        #print(en['filename'],bn,matthews_corrcoef(y_true,y_pred),accuracy_score(y_true,y_pred))\n",
    "            \n",
    "    ax.plot(bnoise, mtr, label=en['filename'],marker='o',markersize=3)\n",
    "    #ax.errorbar(bnoise, mtr, yerr=mtrErr)\n",
    "\n",
    "# add Eden results\n",
    "mtr=[]\n",
    "for bn in bnoise:\n",
    "    y_pred = np.loadtxt('eden/test_pred_eden_'+str(bn)+'.txt',dtype='str')\n",
    "    y_true = np.loadtxt('eden/test_labels_eden_'+str(bn)+'.txt',dtype='str')\n",
    "    mtr.append(pmetricf(y_true,y_pred))\n",
    "    #mtr.append(accuracy_score(y_true,y_pred))\n",
    "    #print('EdeN',bn,matthews_corrcoef(y_true,y_pred),accuracy_score(y_true,y_pred))\n",
    "\n",
    "ax.plot(bnoise, mtr, label='EdeN',marker='o',markersize=3)\n",
    "# add nRC results\n",
    "mtr=[]\n",
    "for bn in bnoise:\n",
    "    y_pred = np.loadtxt('nrc/test_pred_nrc_'+str(bn)+'.txt',dtype='str')\n",
    "    y_true = np.loadtxt('nrc/test_labels_nrc_'+str(bn)+'.txt',dtype='str')\n",
    "    mtr.append(pmetricf(y_true,y_pred))\n",
    "    #print('nRC',bn,matthews_corrcoef(y_true,y_pred),accuracy_score(y_true,y_pred))\n",
    "\n",
    "\n",
    "ax.plot(bnoise, mtr, label='nRC',marker='o',markersize=3)\n",
    "ax.legend()\n",
    "ax.grid(linestyle='--')\n",
    "plt.grid(True)\n",
    "plt.savefig('figs/plot_bnoise-m'+pmetric+'_nl'+str(nl)+'_p'+padd+'.pdf')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots accuracy with different padding schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import *\n",
    "from utils.ExpConfiguration import *\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# CHANGE HERE plot parameters\n",
    "nl=3  # cnn layer to plot\n",
    "bn = 0 # boundary noise\n",
    "#pmetric = 'MCC'\n",
    "#pmetricf = matthews_corrcoef\n",
    "pmetric = 'ACC'\n",
    "pmetricf = accuracy_score\n",
    "\n",
    "\n",
    "y_true = np.argmax(test_labels_bin, axis=1)\n",
    "index = ['New', 'Constant', 'Random']\n",
    "cols = {}\n",
    "colsErr = {}\n",
    "for en in seqEncoders:\n",
    "    y_pred = v_acc_d['New'][str(nl)][en['filename']][str(bn)]\n",
    "    pnew = pmetricf(y_true,y_pred)\n",
    "    pnewInt = 1.96 * sqrt( (pnew * (1 - pnew)) / len(y_pred))\n",
    "\n",
    "    y_pred = v_acc_d['Constant'][str(nl)][en['filename']][str(bn)]\n",
    "    pcns = pmetricf(y_true,y_pred)\n",
    "    pcnsInt = 1.96 * sqrt( (pcns * (1 - pcns)) / len(y_pred))\n",
    "\n",
    "    y_pred = v_acc_d['Random'][str(nl)][en['filename']][str(bn)]\n",
    "    prnd = pmetricf(y_true,y_pred)\n",
    "    prndInt = 1.96 * sqrt( (prnd * (1 - prnd)) / len(y_pred))\n",
    "\n",
    "    errors = [pnewInt,pcnsInt,prndInt]\n",
    "    performance = [pnew,pcns,prnd]\n",
    "    cols.update({en['filename'] : performance})\n",
    "    colsErr.update({en['filename'] : errors})\n",
    "\n",
    "\n",
    "\n",
    "df = pd.DataFrame(cols, index=index)\n",
    "dfErr = pd.DataFrame(colsErr, index=index)\n",
    "ax = df.plot.bar(rot=0,ylim=(0.5,1),yerr=dfErr)\n",
    "ax.grid(linestyle='--')\n",
    "plt.grid(True)\n",
    "ax.legend(loc='lower left')\n",
    "#plt.title('Input padding symbol')\n",
    "plt.ylabel('ACC')\n",
    "plt.savefig('figs/plot-padding.pdf')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots accuracy/MCC with different CNN n. of layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import *\n",
    "from utils.ExpConfiguration import *\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# CHANGE HERE plot parameters\n",
    "bn = 0 # boundary noise\n",
    "padd = 'New'  # padding to plot\n",
    "#pmetric = 'MCC'\n",
    "#pmetricf = matthews_corrcoef\n",
    "pmetric = 'ACC'\n",
    "pmetricf = accuracy_score\n",
    "\n",
    "\n",
    "y_true = np.argmax(test_labels_bin, axis=1)\n",
    "index = ['0', '1', '2','3']\n",
    "cols = {}\n",
    "colsErr = {}\n",
    "for en in seqEncoders:\n",
    "    y_pred = v_acc[str(0)][en['filename']][str(bn)]\n",
    "    p0 = pmetricf(y_true,y_pred)\n",
    "    p0E = 1.96 * sqrt( (p0 * (1 - p0)) / len(y_pred))\n",
    "    y_pred = v_acc[str(1)][en['filename']][str(bn)]\n",
    "    p1 = pmetricf(y_true,y_pred)\n",
    "    p1E = 1.96 * sqrt( (p1 * (1 - p1)) / len(y_pred))\n",
    "\n",
    "    y_pred = v_acc[str(2)][en['filename']][str(bn)]\n",
    "    p2 = pmetricf(y_true,y_pred)\n",
    "    p2E = 1.96 * sqrt( (p2 * (1 - p2)) / len(y_pred))\n",
    "\n",
    "    y_pred = v_acc[str(3)][en['filename']][str(bn)]\n",
    "    p3 = pmetricf(y_true,y_pred)\n",
    "    p3E = 1.96 * sqrt( (p3 * (1 - p3)) / len(y_pred))\n",
    "\n",
    "    \n",
    "    performance = [p0,p1,p2,p3]\n",
    "    errors = [p0E,p1E,p2E,p3E]\n",
    "    cols.update({en['filename'] : performance})\n",
    "    colsErr.update({en['filename'] : errors})\n",
    "\n",
    "\n",
    "\n",
    "df = pd.DataFrame(cols, index=index)\n",
    "dfErr = pd.DataFrame(colsErr, index=index)\n",
    "ax = df.plot.bar(rot=0,ylim=(0.5,1),yerr=dfErr)\n",
    "ax.grid(linestyle='--')\n",
    "plt.grid(True)\n",
    "ax.legend(loc='upper left')\n",
    "\n",
    "#plt.title('CNN number of layers')\n",
    "plt.ylabel('ACC')\n",
    "plt.xlabel('CNN n. of layers')\n",
    "plt.savefig('figs/plot-cnnlayers.pdf')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generates tables with precisions, recalls, and F1-measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tables precision recall f1 and macro/weighted averages \n",
    "# at certain bnoise and n CNN layers \n",
    "\n",
    "from sklearn.metrics import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from utils.ExpConfiguration import *\n",
    "\n",
    "# CHANGE HERE plot parameters\n",
    "nl=3  # cnn layer \n",
    "bn = 0 # boundary noise\n",
    "padd = 'New'  # padding to plot\n",
    "\n",
    "v_acc = v_acc_d[padd]\n",
    "y_true = np.argmax(test_labels_bin, axis=1)\n",
    "y_true = le.inverse_transform(y_true)\n",
    "\n",
    "labels,lcounts = np.unique(y_true,return_counts=True)\n",
    "\n",
    "dfs = {}\n",
    "for en in seqEncoders:\n",
    "    y_pred = v_acc[str(nl)][en['filename']][str(bn)]\n",
    "    y_pred = le.inverse_transform(y_pred)\n",
    "    clf1 = f1_score(y_true,y_pred,average=None,labels=labels)\n",
    "    clf1=np.append(clf1,f1_score(y_true,y_pred,average='weighted'))\n",
    "    clf1=np.append(clf1,f1_score(y_true,y_pred,average='macro'))\n",
    "    clf1=np.append(clf1,f1_score(y_true,y_pred,average='micro'))\n",
    "    dfs[en['filename']] = clf1\n",
    "\n",
    "y_pred = np.loadtxt('eden/test_pred_eden_'+str(bn)+'.txt',dtype='str')\n",
    "y_true = np.loadtxt('eden/test_labels_eden_'+str(bn)+'.txt',dtype='str')\n",
    "clf1 = f1_score(y_true,y_pred,average=None,labels=labels)\n",
    "clf1=np.append(clf1,f1_score(y_true,y_pred,average='weighted'))\n",
    "clf1=np.append(clf1,f1_score(y_true,y_pred,average='macro'))\n",
    "clf1=np.append(clf1,f1_score(y_true,y_pred,average='micro'))\n",
    "dfs['EdeN'] = clf1\n",
    "\n",
    "y_pred = np.loadtxt('nrc/test_pred_nrc_'+str(bn)+'.txt',dtype='str')\n",
    "y_true = np.loadtxt('nrc/test_labels_nrc_'+str(bn)+'.txt',dtype='str')\n",
    "clf1 = f1_score(y_true,y_pred,average=None,labels=labels)\n",
    "clf1=np.append(clf1,f1_score(y_true,y_pred,average='weighted'))\n",
    "clf1=np.append(clf1,f1_score(y_true,y_pred,average='macro'))\n",
    "clf1=np.append(clf1,f1_score(y_true,y_pred,average='micro'))\n",
    "dfs['nRC'] = clf1\n",
    "\n",
    "dfs['Class size'] = np.append(lcounts,['-','-','-'])\n",
    "\n",
    "\n",
    "labels = np.append(labels,['Weigthed avr','Macro avr','Accuracy'])\n",
    "df = pd.DataFrame(dfs,index=labels)\n",
    "#df = df.transpose()\n",
    "#df.astype({('nRC','Class size'): 'int32'})\n",
    "\n",
    "with open('tables/prf-table_bn'+str(bn)+'_nl'+str(nl)+'_p'+padd+'.tex','w') as tf:\n",
    "    tf.write(df.to_latex(float_format=\"{:0.2f}\".format))\n",
    "\n",
    "df.to_excel('tables/SupplementalTableS1.xlsx',float_format=\"%.2f\")\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results with RNAGCN/nRC dataset and improved architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "v_acc_d = {}\n",
    "\n",
    "f = open('results/RNAGCN_nRC_ModelImproved_new.pckl', 'rb')\n",
    "v_acc_d.update({'Improved' : pickle.load(f)})\n",
    "f.close()\n",
    "\n",
    "f = open('results/RNAGCN_nRC_new.pckl', 'rb')\n",
    "v_acc_d.update({'Standard' : pickle.load(f)})\n",
    "f.close()\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from sklearn import preprocessing\n",
    "\n",
    "train_labels=np.load(\"dataset_nRC_train_labels.npy\")\n",
    "test_labels=np.load(\"dataset_nRC_test_labels.npy\")\n",
    "\n",
    "num_classes = len(np.unique(train_labels))\n",
    "print('Total classes: ',num_classes)\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(train_labels)\n",
    "train_labels_num = le.transform(train_labels)\n",
    "test_labels_num = le.transform(test_labels)\n",
    "\n",
    "train_labels_bin = keras.utils.to_categorical(train_labels_num, num_classes)\n",
    "test_labels_bin = keras.utils.to_categorical(test_labels_num, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import *\n",
    "from ExpConfiguration import *\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# CHANGE HERE plot parameters\n",
    "bn = 0 # boundary noise\n",
    "padd = 'New'  # padding to plot\n",
    "#pmetric = 'MCC'\n",
    "#pmetricf = matthews_corrcoef\n",
    "pmetric = 'ACC'\n",
    "pmetricf = accuracy_score\n",
    "\n",
    "\n",
    "y_true = np.argmax(test_labels_bin, axis=1)\n",
    "index = ['Standard', 'Improved']\n",
    "cols = {}\n",
    "for en in seqEncoders:\n",
    "    y_pred = v_acc_d['Standard'][en['filename']][str(bn)]\n",
    "    p0 = pmetricf(y_true,y_pred)\n",
    "    y_pred = v_acc_d['Improved'][en['filename']][str(bn)]\n",
    "    p1 = pmetricf(y_true,y_pred)\n",
    "    \n",
    "    performance = [p0,p1]\n",
    "    cols.update({en['filename'] : performance})\n",
    "\n",
    "\n",
    "\n",
    "df = pd.DataFrame(cols, index=index)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results of rejection experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rejection experiments figures and tables\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import pickle\n",
    "\n",
    "v_acc_d = {}\n",
    "\n",
    "f = open('results/RejectionExperiments_new.pckl', 'rb')\n",
    "outdata = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "test_labels=np.load(\"test_labels.npy\")\n",
    "num_classes = len(np.unique(test_labels))\n",
    "print('Total classes: ',num_classes)\n",
    "\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(test_labels)\n",
    "test_labels_num = le.transform(test_labels)\n",
    "\n",
    "test_labels_bin = keras.utils.to_categorical(test_labels_num, num_classes)\n",
    "\n",
    "y_true = np.argmax(test_labels_bin, axis=1)\n",
    "y_true = le.inverse_transform(y_true)\n",
    "labels,lcounts = np.unique(y_true,return_counts=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification Improvement: Distance between two top probs\n",
    "\n",
    "from sklearn.metrics import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from utils.ExpConfiguration import *\n",
    "\n",
    "\n",
    "df = pd.DataFrame(index=labels)\n",
    "\n",
    "dfs = {}\n",
    "for en in seqEncoders:\n",
    "    idx=[]\n",
    "    avrp_nornd = outdata[en['filename']]['avrp_nornd']\n",
    "    orderp_nornd = outdata[en['filename']]['orderp_nornd']\n",
    "    var_nornd = outdata[en['filename']]['var_nornd']\n",
    "    \n",
    "    for i in range(len(avrp_nornd)):\n",
    "        l1 = avrp_nornd[i,orderp_nornd[i,0]] - 0.6*np.sqrt(var_nornd[i,orderp_nornd[i,0]])\n",
    "        l2 = avrp_nornd[i,orderp_nornd[i,1]] + 0.6*np.sqrt(var_nornd[i,orderp_nornd[i,1]])\n",
    "        D = l1 - l2\n",
    "        if D>0:\n",
    "            idx.append(i)\n",
    "\n",
    "    y_true_rej = y_true[idx]\n",
    "    y_pred = outdata[en['filename']]['y_pred']\n",
    "    y_pred_rej = y_pred[idx]\n",
    "    clf1 = f1_score(y_true_rej,y_pred_rej,average=None,labels=labels)\n",
    "    #clf1=np.append(clf1,f1_score(y_true_rej,y_pred_rej,average='weighted'))\n",
    "    #clf1=np.append(clf1,f1_score(y_true_rej,y_pred_rej,average='macro'))\n",
    "    #clf1=np.append(clf1,f1_score(y_true_rej,y_pred_rej,average='micro'))\n",
    "    dfs[en['filename']] = clf1\n",
    "\n",
    "    labels_idx,lcounts_idx = np.unique(y_true[idx],return_counts=True)\n",
    "\n",
    "    df[en['filename']] = 0\n",
    "    \n",
    "    df.loc[labels , en['filename']] = lcounts\n",
    "    \n",
    "    df.loc[labels_idx , en['filename']] = df.loc[labels_idx , en['filename']]-lcounts_idx\n",
    "    \n",
    "    df.loc[labels , en['filename']] = 100*df.loc[labels , en['filename']]/lcounts\n",
    "\n",
    "    print(' & %s & %0.2f & %0.2f & %0.2f & %0.2f' % (en['filename'], accuracy_score(y_true_rej,y_pred_rej),\n",
    "                                                                  cohen_kappa_score(y_true_rej,y_pred_rej),\n",
    "                                                                  matthews_corrcoef(y_true_rej,y_pred_rej),\n",
    "                                                                  100*(np.sum(lcounts)-np.sum(lcounts_idx))/np.sum(lcounts)))\n",
    "\n",
    "\n",
    "dfD = pd.DataFrame(dfs,index=labels)\n",
    "dfDrej = df\n",
    "\n",
    "dfDrej.style.background_gradient(cmap='Blues')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification Improvement: Entropy\n",
    "\n",
    "from sklearn.metrics import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from utils.ExpConfiguration import *\n",
    "\n",
    "\n",
    "df = pd.DataFrame(index=labels)\n",
    "\n",
    "dfs = {}\n",
    "for en in seqEncoders:\n",
    "    idx=[]\n",
    "    hp_nornd = outdata[en['filename']]['hp_nornd']\n",
    "    \n",
    "    for i in range(len(hp_nornd)):\n",
    "        H = hp_nornd[i]\n",
    "        if H < -np.log2(1/num_classes)/3:\n",
    "            idx.append(i)\n",
    "\n",
    "    y_true_rej = y_true[idx]\n",
    "    y_pred = outdata[en['filename']]['y_pred']\n",
    "    y_pred_rej = y_pred[idx]\n",
    "    clf1 = f1_score(y_true_rej,y_pred_rej,average=None,labels=labels)\n",
    "    #clf1=np.append(clf1,f1_score(y_true_rej,y_pred_rej,average='weighted'))\n",
    "    #clf1=np.append(clf1,f1_score(y_true_rej,y_pred_rej,average='macro'))\n",
    "    #clf1=np.append(clf1,f1_score(y_true_rej,y_pred_rej,average='micro'))\n",
    "    dfs[en['filename']] = clf1\n",
    "\n",
    "    labels_idx,lcounts_idx = np.unique(y_true[idx],return_counts=True)\n",
    "\n",
    "    df[en['filename']] = 0\n",
    "    \n",
    "    df.loc[labels , en['filename']] = lcounts\n",
    "    \n",
    "    df.loc[labels_idx , en['filename']] = df.loc[labels_idx , en['filename']]-lcounts_idx\n",
    "    \n",
    "    df.loc[labels , en['filename']] = 100*df.loc[labels , en['filename']]/lcounts\n",
    "    \n",
    "    print(' & %s & %0.2f & %0.2f & %0.2f & %0.2f' % (en['filename'], accuracy_score(y_true_rej,y_pred_rej),\n",
    "                                                                  cohen_kappa_score(y_true_rej,y_pred_rej),\n",
    "                                                                  matthews_corrcoef(y_true_rej,y_pred_rej),\n",
    "                                                                  100*(np.sum(lcounts)-np.sum(lcounts_idx))/np.sum(lcounts)))\n",
    "\n",
    "\n",
    "\n",
    "dfH = pd.DataFrame(dfs,index=labels)\n",
    "dfHrej = df\n",
    "\n",
    "dfH.style.background_gradient(cmap='Blues')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, (ax0, ax1) = plt.subplots(1, 2, constrained_layout=True)\n",
    "\n",
    "fig.set_size_inches(9,9)\n",
    "dff1 = dfH\n",
    "df = dfHrej\n",
    "\n",
    "c0 = ax0.imshow(dff1,cmap=\"Blues\",vmax=1,vmin=0,aspect='auto')\n",
    "c1 = ax1.imshow(df,cmap=\"Oranges\",vmax=100,vmin=0,aspect='auto')\n",
    "\n",
    "#ax0.get_yaxis().set_visible(False)\n",
    "\n",
    "\n",
    "ax0.set_yticks(np.arange(len(dff1.index)))\n",
    "ax0.set_yticklabels(dff1.index)\n",
    "ax0.set_ylim(28.5,-0.5)\n",
    "\n",
    "ax0.set_xticks(np.arange(len(dff1.columns)))\n",
    "ax0.set_xticklabels(dff1.columns,fontsize=12)\n",
    "\n",
    "ax1.get_yaxis().set_visible(False)\n",
    "#ax1.set_yticks(np.arange(1,len(df.index)))\n",
    "#ax1.set_yticklabels(df.index)\n",
    "ax1.set_xticks(np.arange(len(df.columns)))\n",
    "ax1.set_xticklabels(df.columns,fontsize=12)\n",
    "\n",
    "\n",
    "fig.suptitle('Entropy Estimator', fontsize=16)\n",
    "\n",
    "# Rotate the tick labels and set their alignment.\n",
    "plt.setp(ax0.get_xticklabels(), rotation=90, ha=\"right\",\n",
    "         rotation_mode=\"anchor\",verticalalignment='center')\n",
    "plt.setp(ax1.get_xticklabels(), rotation=90, ha=\"right\",\n",
    "         rotation_mode=\"anchor\",verticalalignment='center')\n",
    "plt.setp(ax0.get_yticklabels(), rotation=0, ha=\"right\",\n",
    "         rotation_mode=\"anchor\",verticalalignment='center')\n",
    "plt.setp(ax1.get_yticklabels(), rotation=0, ha=\"right\",\n",
    "         rotation_mode=\"anchor\",verticalalignment='center')\n",
    "\n",
    "\n",
    "\n",
    "#cax2 = ax1_divider.append_axes(\"top\", size=\"7%\", pad=\"2%\")\n",
    "#cb2 = fig.colorbar(im2, cax=cax2, orientation=\"horizontal\")\n",
    "# change tick position to top. Tick position defaults to bottom and overlaps\n",
    "# the image.\n",
    "#cax2.xaxis.set_ticks_position(\"top\")\n",
    "\n",
    "#c = ax1.pcolor(df)\n",
    "cbar1 = fig.colorbar(c0, ax=ax0,cmap='Blues',location='top')\n",
    "cbar1.set_label('F1-measure', rotation=0, fontsize=12)\n",
    "cbar2 = fig.colorbar(c1, ax=ax1,cmap='Oranges',location='top')\n",
    "cbar2.set_label('% of rejected samples', rotation=0, fontsize=12)\n",
    "\n",
    "\n",
    "\n",
    "plt.savefig('figs/plot_rej_improvement_H.pdf')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, (ax0, ax1, ax2, ax3) = plt.subplots(1, 4, constrained_layout=True)\n",
    "\n",
    "fig.set_size_inches(9,9)\n",
    "dff1 = dfH\n",
    "df = dfHrej\n",
    "\n",
    "c0 = ax0.imshow(dff1,cmap=\"Blues\",vmax=1,vmin=0,aspect='auto')\n",
    "c1 = ax1.imshow(df,cmap=\"Oranges\",vmax=100,vmin=0,aspect='auto')\n",
    "\n",
    "#ax0.get_yaxis().set_visible(False)\n",
    "\n",
    "\n",
    "ax0.set_yticks(np.arange(len(dff1.index)))\n",
    "ax0.set_yticklabels(dff1.index)\n",
    "ax0.set_ylim(28.5,-0.5)\n",
    "ax0.set_title(\"Entropy Estimator\")\n",
    "\n",
    "#ax0.get_xaxis().set_visible(False)\n",
    "ax0.set_xticks(np.arange(len(dff1.columns)))\n",
    "ax0.set_xticklabels(dff1.columns,fontsize=12)\n",
    "\n",
    "ax1.get_yaxis().set_visible(False)\n",
    "#ax1.get_xaxis().set_visible(False)\n",
    "#ax1.set_yticks(np.arange(1,len(df.index)))\n",
    "#ax1.set_yticklabels(df.index)\n",
    "ax1.set_xticks(np.arange(len(df.columns)))\n",
    "ax1.set_xticklabels(df.columns,fontsize=12)\n",
    "\n",
    "\n",
    "\n",
    "#fig.suptitle('Entropy Estimator', fontsize=16)\n",
    "\n",
    "# Rotate the tick labels and set their alignment.\n",
    "plt.setp(ax0.get_xticklabels(), rotation=90, ha=\"right\",\n",
    "         rotation_mode=\"anchor\",verticalalignment='center')\n",
    "plt.setp(ax1.get_xticklabels(), rotation=90, ha=\"right\",\n",
    "         rotation_mode=\"anchor\",verticalalignment='center')\n",
    "plt.setp(ax0.get_yticklabels(), rotation=0, ha=\"right\",\n",
    "         rotation_mode=\"anchor\",verticalalignment='center')\n",
    "plt.setp(ax1.get_yticklabels(), rotation=0, ha=\"right\",\n",
    "         rotation_mode=\"anchor\",verticalalignment='center')\n",
    "\n",
    "\n",
    "\n",
    "#cax2 = ax1_divider.append_axes(\"top\", size=\"7%\", pad=\"2%\")\n",
    "#cb2 = fig.colorbar(im2, cax=cax2, orientation=\"horizontal\")\n",
    "# change tick position to top. Tick position defaults to bottom and overlaps\n",
    "# the image.\n",
    "#cax2.xaxis.set_ticks_position(\"top\")\n",
    "\n",
    "#c = ax1.pcolor(df)\n",
    "cbar1 = fig.colorbar(c0, ax=[ax0,ax1],cmap='Blues',shrink=0.6,location='top')\n",
    "cbar1.set_label('F1-measure', rotation=0, fontsize=12)\n",
    "cbar2 = fig.colorbar(c1, ax=[ax2,ax3],cmap='Oranges',shrink=0.6,location='top')\n",
    "cbar2.set_label('% of rejected samples', rotation=0, fontsize=12)\n",
    "\n",
    "\n",
    "dff1 = dfD\n",
    "df = dfDrej\n",
    "\n",
    "c2 = ax2.imshow(dff1,cmap=\"Blues\",vmax=1,vmin=0,aspect='auto')\n",
    "c3 = ax3.imshow(df,cmap=\"Oranges\",vmax=100,vmin=0,aspect='auto')\n",
    "\n",
    "ax2.get_yaxis().set_visible(False)\n",
    "#ax2.set_yticks(np.arange(len(dff1.index)))\n",
    "#ax2.set_yticklabels(dff1.index)\n",
    "#ax2.set_ylim(28.5,-0.5)\n",
    "\n",
    "ax2.set_xticks(np.arange(len(dff1.columns)))\n",
    "ax2.set_xticklabels(dff1.columns,fontsize=12)\n",
    "ax2.set_title(\"Distance Estimator\")\n",
    "\n",
    "\n",
    "ax3.get_yaxis().set_visible(False)\n",
    "#ax1.set_yticks(np.arange(1,len(df.index)))\n",
    "#ax1.set_yticklabels(df.index)\n",
    "ax3.set_xticks(np.arange(len(df.columns)))\n",
    "ax3.set_xticklabels(df.columns,fontsize=12)\n",
    "\n",
    "\n",
    "\n",
    "#fig.suptitle('Entropy Estimator', fontsize=16)\n",
    "\n",
    "# Rotate the tick labels and set their alignment.\n",
    "plt.setp(ax2.get_xticklabels(), rotation=90, ha=\"right\",\n",
    "         rotation_mode=\"anchor\",verticalalignment='center')\n",
    "plt.setp(ax3.get_xticklabels(), rotation=90, ha=\"right\",\n",
    "         rotation_mode=\"anchor\",verticalalignment='center')\n",
    "plt.setp(ax2.get_yticklabels(), rotation=0, ha=\"right\",\n",
    "         rotation_mode=\"anchor\",verticalalignment='center')\n",
    "plt.setp(ax3.get_yticklabels(), rotation=0, ha=\"right\",\n",
    "         rotation_mode=\"anchor\",verticalalignment='center')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.savefig('figs/plot_rej_improvement_HD.pdf')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from matplotlib import pyplot\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "en = '2mer'\n",
    "\n",
    "maxp_rnd = outdata[en]['maxp_rnd']\n",
    "maxp_nornd = outdata[en]['maxp_nornd']\n",
    "\n",
    "pred_score = np.concatenate([maxp_nornd, maxp_rnd])\n",
    "true_class = np.concatenate([np.ones(len(maxp_nornd)), np.zeros(len(maxp_rnd))])\n",
    "\n",
    "\n",
    "ns_auc = roc_auc_score(true_class, pred_score)\n",
    "ns_fpr, ns_tpr, _ = roc_curve(true_class, pred_score)\n",
    "\n",
    "\n",
    "fig, axs = pyplot.subplots(1, 2, constrained_layout=True,figsize=(6,3))\n",
    "fig.suptitle(en, fontsize=16)\n",
    "\n",
    "#pyplot.figure(figsize=(6,3))\n",
    "axs[0].plot(ns_fpr, ns_tpr, linestyle='-')\n",
    "axs[0].text(0.6, 0.2, 'AUC = %.2f' % ns_auc)\n",
    "axs[0].grid(linestyle='--')\n",
    "axs[0].set_xlabel('False Positive Rate')\n",
    "axs[0].set_ylabel('True Positive Rate')\n",
    "\n",
    "axs[1].hist([maxp_nornd, maxp_rnd], bins='auto', density=True, alpha=0.5, label=['Functional seq', 'Random seq'])\n",
    "\n",
    "axs[1].legend(loc='upper left')\n",
    "axs[1].grid(linestyle='--')\n",
    "axs[1].set_xlabel(r'Uncertainty (estimator $p_{max}$)')\n",
    "#\n",
    "pyplot.savefig('figs/plot_rejection_avrp_'+en+'.pdf')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from matplotlib import pyplot\n",
    "import numpy as np\n",
    "\n",
    "en = '2mer'\n",
    "\n",
    "maxfp_rnd = outdata[en]['maxfp_rnd']\n",
    "maxfp_nornd = outdata[en]['maxfp_nornd']\n",
    "\n",
    "pred_score = np.concatenate([maxfp_nornd, maxfp_rnd])\n",
    "true_class = np.concatenate([np.ones(len(maxfp_nornd)), np.zeros(len(maxfp_rnd))])\n",
    "\n",
    "\n",
    "\n",
    "ns_auc = roc_auc_score(true_class, pred_score)\n",
    "ns_fpr, ns_tpr, _ = roc_curve(true_class, pred_score)\n",
    "\n",
    "\n",
    "fig, axs = pyplot.subplots(1, 2, constrained_layout=True,figsize=(6,3))\n",
    "fig.suptitle(en, fontsize=16)\n",
    "\n",
    "#pyplot.figure(figsize=(6,3))\n",
    "axs[0].plot(ns_fpr, ns_tpr, linestyle='-')\n",
    "axs[0].text(0.6, 0.2, 'AUC = %.2f' % ns_auc)\n",
    "axs[0].grid(linestyle='--')\n",
    "axs[0].set_xlabel('False Positive Rate')\n",
    "axs[0].set_ylabel('True Positive Rate')\n",
    "\n",
    "axs[1].hist([maxfp_nornd, maxfp_rnd], bins='auto', density=True, alpha=0.5, label=['Functional seq', 'Random seq'])\n",
    "\n",
    "axs[1].legend(loc='upper left')\n",
    "axs[1].grid(linestyle='--')\n",
    "axs[1].set_xlabel(r'Uncertainty (estimator $f_{max}$)')\n",
    "#\n",
    "pyplot.savefig('figs/plot_rejection_fp_'+en+'.pdf')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from matplotlib import pyplot\n",
    "import numpy as np\n",
    "\n",
    "en = '1mer'\n",
    "\n",
    "fig, axs = pyplot.subplots(1, 3, constrained_layout=True,figsize=(9,3))\n",
    "#fig.suptitle(en, fontsize=16)\n",
    "\n",
    "hp_rnd = outdata[en]['hp_rnd']\n",
    "hp_nornd = outdata[en]['hp_nornd']\n",
    "\n",
    "pred_score = np.concatenate([-hp_nornd, -hp_rnd])\n",
    "true_class = np.concatenate([np.ones(len(hp_nornd)), np.zeros(len(hp_rnd))])\n",
    "\n",
    "ns_auc = roc_auc_score(true_class, pred_score)\n",
    "ns_fpr, ns_tpr, _ = roc_curve(true_class, pred_score)\n",
    "\n",
    "axs[0].plot(ns_fpr, ns_tpr, linestyle='-',label='Entropy')\n",
    "axs[0].text(0.6, 0.4, 'H AUC = %.2f' % ns_auc)\n",
    "\n",
    "\n",
    "D_nornd=[]\n",
    "avrp_nornd = outdata[en]['avrp_nornd']\n",
    "orderp_nornd = outdata[en]['orderp_nornd']\n",
    "var_nornd = outdata[en]['var_nornd']\n",
    "    \n",
    "for i in range(len(avrp_nornd)):\n",
    "    l1 = avrp_nornd[i,orderp_nornd[i,0]] - 0.6*np.sqrt(var_nornd[i,orderp_nornd[i,0]])\n",
    "    l2 = avrp_nornd[i,orderp_nornd[i,1]] + 0.6*np.sqrt(var_nornd[i,orderp_nornd[i,1]])\n",
    "    D = l1 - l2\n",
    "    D_nornd.append(D)\n",
    "\n",
    "D_rnd=[]\n",
    "avrp_rnd = outdata[en]['avrp_rnd']\n",
    "orderp_rnd = outdata[en]['orderp_rnd']\n",
    "var_rnd = outdata[en]['var_rnd']\n",
    "    \n",
    "for i in range(len(avrp_rnd)):\n",
    "    l1 = avrp_rnd[i,orderp_rnd[i,0]] - 0.6*np.sqrt(var_rnd[i,orderp_rnd[i,0]])\n",
    "    l2 = avrp_rnd[i,orderp_rnd[i,1]] + 0.6*np.sqrt(var_rnd[i,orderp_rnd[i,1]])\n",
    "    D = l1 - l2\n",
    "    D_rnd.append(D)\n",
    "\n",
    "pred_score = np.concatenate([D_nornd, D_rnd])\n",
    "true_class = np.concatenate([np.ones(len(D_nornd)), np.zeros(len(D_rnd))])\n",
    "\n",
    "ns_auc = roc_auc_score(true_class, pred_score)\n",
    "ns_fpr, ns_tpr, _ = roc_curve(true_class, pred_score)\n",
    "\n",
    "\n",
    "axs[0].plot(ns_fpr, ns_tpr, linestyle='-',label='Distance')\n",
    "axs[0].text(0.6, 0.3, 'D AUC = %.2f' % ns_auc)\n",
    "axs[0].grid(linestyle='--')\n",
    "axs[0].set_xlabel('False Positive Rate')\n",
    "axs[0].set_ylabel('True Positive Rate')\n",
    "\n",
    "axs[0].legend(loc='lower right')\n",
    "\n",
    "\n",
    "axs[1].hist([hp_nornd, hp_rnd], bins='auto', density=True, alpha=0.5, label=['Functional seq', 'Random seq'])\n",
    "\n",
    "axs[1].legend(loc='upper right')\n",
    "axs[1].grid(linestyle='--')\n",
    "axs[1].set_xlabel(r'Uncertainty (estimator $H$)')\n",
    "\n",
    "\n",
    "axs[2].hist([D_nornd, D_rnd], bins='auto', density=True, alpha=0.5, label=['Functional seq', 'Random seq'])\n",
    "axs[2].legend(loc='upper left')\n",
    "axs[2].grid(linestyle='--')\n",
    "axs[2].set_xlabel(r'Uncertainty (estimator $D$)')\n",
    "\n",
    "\n",
    "#\n",
    "pyplot.savefig('figs/plot_rejection_HD.pdf')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from matplotlib import pyplot\n",
    "import numpy as np\n",
    "\n",
    "en = '1mer'\n",
    "\n",
    "D_nornd=[]\n",
    "avrp_nornd = outdata[en]['avrp_nornd']\n",
    "orderp_nornd = outdata[en]['orderp_nornd']\n",
    "var_nornd = outdata[en]['var_nornd']\n",
    "    \n",
    "for i in range(len(avrp_nornd)):\n",
    "    l1 = avrp_nornd[i,orderp_nornd[i,0]] - 0.6*np.sqrt(var_nornd[i,orderp_nornd[i,0]])\n",
    "    l2 = avrp_nornd[i,orderp_nornd[i,1]] + 0.6*np.sqrt(var_nornd[i,orderp_nornd[i,1]])\n",
    "    D = l1 - l2\n",
    "    D_nornd.append(D)\n",
    "\n",
    "D_rnd=[]\n",
    "avrp_rnd = outdata[en]['avrp_rnd']\n",
    "orderp_rnd = outdata[en]['orderp_rnd']\n",
    "var_rnd = outdata[en]['var_rnd']\n",
    "    \n",
    "for i in range(len(avrp_rnd)):\n",
    "    l1 = avrp_rnd[i,orderp_rnd[i,0]] - 0.6*np.sqrt(var_rnd[i,orderp_rnd[i,0]])\n",
    "    l2 = avrp_rnd[i,orderp_rnd[i,1]] + 0.6*np.sqrt(var_rnd[i,orderp_rnd[i,1]])\n",
    "    D = l1 - l2\n",
    "    D_rnd.append(D)\n",
    "\n",
    "pred_score = np.concatenate([D_nornd, D_rnd])\n",
    "true_class = np.concatenate([np.ones(len(D_nornd)), np.zeros(len(D_rnd))])\n",
    "\n",
    "ns_auc = roc_auc_score(true_class, pred_score)\n",
    "ns_fpr, ns_tpr, _ = roc_curve(true_class, pred_score)\n",
    "\n",
    "\n",
    "fig, axs = pyplot.subplots(2, 1, constrained_layout=True,figsize=(3,6))\n",
    "#fig.suptitle(en, fontsize=16)\n",
    "\n",
    "#pyplot.figure(figsize=(6,3))\n",
    "axs[0].plot(ns_fpr, ns_tpr, linestyle='-')\n",
    "axs[0].text(0.6, 0.2, 'AUC = %.2f' % ns_auc)\n",
    "axs[0].grid(linestyle='--')\n",
    "axs[0].set_xlabel('False Positive Rate')\n",
    "axs[0].set_ylabel('True Positive Rate')\n",
    "\n",
    "axs[1].hist([D_nornd, D_rnd], bins='auto', density=True, alpha=0.5, label=['Functional seq', 'Random seq'])\n",
    "\n",
    "axs[1].legend(loc='upper left')\n",
    "axs[1].grid(linestyle='--')\n",
    "axs[1].set_xlabel(r'Uncertainty (estimator $D$)')\n",
    "#\n",
    "pyplot.savefig('figs/plot_rejection_D_'+en+'.pdf')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
